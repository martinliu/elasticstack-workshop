---
title: "使用 Kibana 分析开源项目的KPI"
date: 2017-10-17T15:26:15Z
draft: false
weight: 23
---

开源项目的火热程度取决于技术创新+行业趋势，很多风口浪尖的开源项目确实非常吸引人。GitHub中的项目好不好，是不是光数星星就够了？该如何洞察一个项目的社区参与程度？如何判断核心开发团队的工作效能？为了回答这些深度的问题，还需要对其进行深度的分析。本教程教你使用 Elastic Stack 构建 GitHub 上开源项目的关键分析指标看板，在 Kibana 中详细分析项目的各个关键维度。 


{{< panel title="学习目标" >}}
1）使用  zip / tar.gz 软件包在本地搭建 Elastic Stack 开发环境；2） 使用 Filebeat 的 HTTP JSON inputedit 插件从 GitHub API 下载分析数据；3）在 Kibana 中查询和分析文档数据，为图形化展示做好准备；4）掌握 Kibana 数据分析展示看板的基本操作。{{< /panel >}}


## 准备工作

1. 在浏览器中打开网址 https://www.elastic.co/downloads/past-releases#elasticsearch ； 下载 elasticsearch-7.17.0-darwin-aarch64.tar.gz 、kibana-7.17.0-darwin-aarch64.tar.gz 、 filebeat-7.17.0-darwin-x86_64.tar.gz 软件包。请自行选择正确的平台，以上 darwin-aarch64 软件包适用于 moacOS M1 平台；而 darwin-x86_64 软件包适用于 macOS Intel 平台。本视频演示的是下载 Windows 平台的 zip软件包
2. 在 GitHub 中创建 personal token，用于提升访问 GitHub Api 下载的数据条数限制

![github personal access token](/images/personal_access_token.png)

保存这个界面中的类似于这样的个人访问令牌 `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`


## 启动开发环境

**启动 Elasticsearch 服务器**：下面是在 macOS M1 平台上解压缩 Elasticsearch 软件包，进入解压后的软件包的目录，使用其可执行文件启动服务。

```sh
tar zxvf elasticsearch-7.17.0-darwin-aarch64.tar.gz
cd elasticsearch-7.17.0
bin/elasticsearch -Enode.name=mac-m1
```

如果是在 Windows 平台上，使用系统带的解压缩软件将 zip 软件包解压。

**启动 Kibana 服务器**：

```sh
tar zxvf kibana-7.17.0-darwin-aarch64.tar.gz
cd kibana-7.17.0-darwin-aarch64
bin/kibana
```

这样就启动了 Kibana 服务器，进入解压后的软件包的目录，使用其可执行文件启动服务。它默认会连接上一步所启动的 Elasticsearch 服务器，在这个开发环境中，默认没有启用用户名和密码认证。

**准备 Filebeat 的运行环境**：解压缩 Filebeat 软件包，进入解压后的目录，备份默认的 `filebeat.yml` 重命名为 `filebeat.yml.bk`。

```sh
tar zxvf filebeat-7.17.0-darwin-x86_64.tar.gz
cd filebeat-7.17.0-darwin-x86_64
mv filebeat.yml filebeat.yml.bk
```

**初始化 Kibana 的配置**。在浏览器中输入 `http://localhost:5601` ，在登陆了 Kibana 之后，点击左上交的图标，打开左侧隐藏菜单，找到 Stack Management --> Kibana --> Advanced Setting 页面中的这两个选项。

* 打开 “Dark mode” 选项，**打开暗黑模式界面**。

![kibana-dark-mode](/images/kibana-dark-mode.png)

* 设置 “Document Explorer or classic view” 选项。**关闭默认的 classic 经典视图模式**。

![doc-explorer](/images/doc-explorer.png)

{{< panel status="success" title="自查点" >}}
已经成功的将 Elasticsearch 和 Kibana 在本机上，启动并运行正常。可以在浏览器中访问 Kibana 的界面。浏览若干重要的功能界面：Discover、Dashboard、DevTools、Stack Management等。
{{< /panel >}}


## 下载参考配置文件

为了降低学习Kibana数据看板的搭建门槛，请下载准备好的示例文件。

{{< button status="success" icon="fas fa-cloud-download-alt" url="https://github.com/martinliu/sdp-dashboard/archive/refs/heads/main.zip" >}}下载{{< /button >}}

解压以上的zip文件后，其中的三个文件会被使用到：

1. filebeat.yml ： 完整的 Filebeat 配置参数模版。用它替换 Filebeat 的默认配置文件。本示例配置文件分析的是 OpenShift 的 origin 项目，是一个非常火热的项目。需要几天时间才能完整下载所有数据，也可以根据需要将其替换。
2. add_gh_fields.json ： GitHub项目属性扩展字段定义。将其放入 Filebeat 程序文件夹的目录中。
3. export-v1.0.ndjson ： Kibana 可视化数据分析看板模版。将其倒入到新安装的 Kibana 中。

以上三个文件适用于任何操作系统，

## 使用 Filebeat 同步数据

Filebeat 包含多种数据摄入模块：

* AWS CloudWatch
* AWS S3
* Azure Event Hub
* Cloud Foundry
* Container
* filestream
* GCP Pub/Sub
* HTTP Endpoint
* HTTP JSON
* journald
* Kafka
* Log
* MQTT
* NetFlow
* Office 365 Management Activity API
* Redis
* Stdin
* Syslog
* TCP
* UDP

以上模块已经可以覆盖不少的数据源，灵活使用这些模块，就可以将 Filebeat 当作一个 mini 版的 Logstash 使用，当然 Logstash 包含了更丰富的摄入模块和数据处理能力。

### 获取开源项目概要信息

本教程提供的示例配置文件，采集的是 OpenShift 的项目数据，你可以替换为你的目标分析项目。将下载示例配置文件中的 `add_gh_fields.json` 和 `filebeat.yml` 放入 Filebeat 的目录中 。

使用微软的 Virtual Studio Code 编辑器【或者其他的纯文本编辑器】打开 `filebeat.yml` 示例文件。查看该示例文件，并且**删除掉中间的大部分内容**「**或者注释掉这部分先不用的代码**」，留下剩余的部分做初始化测试。留下的部分如下所示。 并使用上面准备好的 GitHub Personal Access Token 替换配置文件中第21行中对应的参数。

```yml
###################### Filebeat Configuration Example #########################
# ============================== Filebeat inputs ===============================
filebeat.inputs:
# ============================== 建议先获取这一部分的数据  ===============================
# overall, contributors, releases, languages, tags 的数据少且重要，可以手工确认之后在做第二部分

# 获取项目的概要信息 - overall
# GET /repos/{owner}/{repo}
- type: httpjson
  interval: 120m
  config_version: 2
  request.url: https://api.github.com/repos/openshift/origin
  request.method: GET 
  # 在请求中增加个人认证令牌，提高数据获取条数限制
  request.transforms:
    - set:
        target: header.Authorization
        value: 'token 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'
  # 使用数据处理器，修整数据
  processors:
    # 优化分析：增加方便搜索分析的字段  
    - add_fields: 
        fields:
          project: origin
          kpi: overall
    # 解码裸json结果：将 message 字段中的 json 内容解码为多个字段
    - decode_json_fields: 
        fields: ["message"]
        target: "json"

# ======================= Elasticsearch template setting =======================
# 配置索引模版基础属性
setup.template.overwrite: true
setup.template.settings:
  index.number_of_shards: 1
  number_of_replicas: 0
  index.mapping.total_fields.limit: 5000

setup.template.name: "filebeat"
setup.template.pattern: "filebeat-*"
setup.template.fields: "fields.yml"

# 修改部分 json 字段默认的数据类型
setup.template.json.enabled: true
setup.template.json.path: "add_gh_fields.json"
setup.template.json.name: "add_gh_fields"

# 禁用ilm功能
setup.ilm.enabled: false
# ================================== General ===================================
name: github-crawler
# ================================== Outputs ===================================
# ---------------------------- Elasticsearch Output ----------------------------
output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "filebeat-7"
# ================================= Processors =================================
# 删除 Filebeeat 采集到的无关数据，节省存储空间
processors:
 - drop_fields:
     fields: ["ecs", "agent", "input", "host", "message"]    
# ================================== Logging ===================================
logging.level: debug
# ============================= X-Pack Monitoring ==============================
# monitoring.enabled: true     
```

以上配置文件仅使用了必要的参数选项，目的是实现开源项目概要参数信息的抓取，并测试 Filebeat 的可用性(filebeat-7.17.0-darwin-x86_64.tar.gz是可以正常运行在macOS M1平台的)。

不论是 macOS，Windows 还是 Linux ，打开命令行工具，进入 Filebeat 的解压缩目录执行下面的测试。

```sh
➜  filebeat-7.17.0-darwin-x86_64 ./filebeat test output
elasticsearch: http://localhost:9200...
  parse url... OK
  connection...
    parse host... OK
    dns lookup... OK
    addresses: ::1, 127.0.0.1
    dial up... OK
  TLS... WARN secure connection disabled
  talk to server... OK
  version: 7.17.0

```

命令 `./filebeat test output` 的返回结果表明：Filebeat 可以正常连接本机的 Elasticsearch 服务器。在Windows操作系统中，需要将 `filebeat` 可执行文件名换成 `filebeat.exe` ，后面的命令行参数不变。如果 filebeat.yml 配置文件的语法错误，这个命令中会报错。

### GitHub API 的限流和限速

使用 Filebeat 作为客户端访问 GitHub API需要了解的重要信息都在官方文档上：[点这里](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting)；本节内容是必须要理解的内容，必须要在运行 Filebeat 的操作系统上做的测试和操作。

* 认证用户每小时的请求发送配额是 5000 次；对于比较流行的项目而言，issue 或 pull request 任意一类都可以轻易超过这个数值，建议使用 since 参数先拉取近期的数据分析，然后逐步拉取旧数据。
* 企业版用户的配额高一些，每个用户，每小时的配额是 15000 次。
* 未认证用户，每小时限制在60次请求。
* 建议在大概分析了 overall, contributors, releases, languages, tags，issues 和 pull request 等数据的条数后，在逐渐的开始逐项的采集各类数据。
* 可以用加大 page size 参数 【per_page 100是最大】的方式降低需要发出的请求次数。page size 越小，采集的项目数据量越大，越可能碰到被限流的情况。


检查当前用户是否被限流的方法。在命令行里运行这条命令 `curl -u martinliu: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 -I https://api.github.com/users/octocat` ； 这个命令中  `-u martinliu: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` 包含了用户名和个人访问令牌（PAT）。

```sh
➜ curl -u martinliu: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 -I https://api.github.com/users/octocat
HTTP/2 200
server: GitHub.com
date: Thu, 17 Mar 2022 11:01:02 GMT
content-type: application/json; charset=utf-8
content-length: 1335
cache-control: private, max-age=60, s-maxage=60
vary: Accept, Authorization, Cookie, X-GitHub-OTP
etag: "c9c3cea653b1e722852a41f65a43a5e969c0722c81525b3f1a27f7678269c6ba"
last-modified: Tue, 22 Feb 2022 15:07:13 GMT
x-oauth-scopes: public_repo, repo:status
x-accepted-oauth-scopes:
github-authentication-token-expiration: 2022-04-15 06:47:45 UTC
x-github-media-type: github.v3; format=json
x-ratelimit-limit: 5000
x-ratelimit-remaining: 4440
x-ratelimit-reset: 1647516784
x-ratelimit-used: 560
x-ratelimit-resource: core
access-control-expose-headers: ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset
access-control-allow-origin: *
strict-transport-security: max-age=31536000; includeSubdomains; preload
x-frame-options: deny
x-content-type-options: nosniff
x-xss-protection: 0
referrer-policy: origin-when-cross-origin, strict-origin-when-cross-origin
content-security-policy: default-src 'none'
vary: Accept-Encoding, Accept, X-Requested-With
x-github-request-id: 4664:4A3B:121A47C:12EB215:623314EE
```

结果中的这几个参数最重要：

```sh
* x-ratelimit-limit: 是 5000 表明用户名和PAT是正确的，如果是 60 表明，这个用户名和token组合错误，为认证用户的限制是 60。
* x-ratelimit-remaining: 4440 当前这个小时里的剩余API请求数。
* x-ratelimit-reset: 1647516784 下一次访问限制被重置的时间。
```

在采集数据之前，确认能看到类似于以上的数据，否则不要继续做下面的步骤。

基于 GitHub API 使用的限流特性，建议分类，分时段采集数据，避免发生被限流的状况。

在使用 Filebeat 的 HTTP JSON 模块采集 GitHub API数据的过程中很难不会碰到被限流的情况（请求），在被限流的时候，可以在 filebeat 的滚动日志中看到类似下面的信息。

```sh
2022-03-17T11:22:53.262+0800	ERROR	[input.httpjson-stateless]	v2/input.go:115	Error while processing http request: failed to execute http client.Do: server responded with status code 403: {"message":"Resource protected by organization SAML enforcement. You must grant your Personal Access token access to this organization.","documentation_url":"https://docs.github.com/articles/authenticating-to-a-github-organization-with-saml-single-sign-on/"}	{"id": "BF2B0C218CAC3BA0", "input_url": "https://api.github.com/repos/elastic/beats/releases"}
```

以上信息表明，你当前的账户访问被限流了，你需要等一个小时后，在让 Filebeat 继续采集数据。

{{< panel status="success" title="自查点" >}}
你已经理解了 GitHub API 访问流控的基本知识。完成了在正式采集数据前的必要测试，已经确认当前用户名和PAT是正确无误。在浏览器中提前测试 filebeat.yml 配置文件中的 request.rul，确保所有 URL 都可以正常返回结果。 可选的测试：在操作系统上用clone的方式下载一个自己的项目，修改某个文件后，将更新push回去，确保和 GitHub 网站的服务是正常的。
{{< /panel >}}

下一步开始进行数据初始化前的准备基础准备工作。

### 采集并确认部分项目数据

在导入数据之前，需要完成下面这几项任务：

1. 导入为大家准备好的 Kibana 配置文件。
2. 在配置文件中只开启 overall 部分的数据采集
3. 首次启动 Filebeat ，并停止它
4. 在 Kibana 中确认采集到的数据
5. 为了大批量数据采集，修改索引字段数量上限

下面详细讲解操作过程。

在解压缩后的 zip 文件中，找到 export-v1.0.ndjson 文件；在浏览器中打开 Kibana 的界面，点击左侧菜单：需要导入示例 Kibana 可视化对象。

点击展开左侧菜单，选择： Management -->  Stack Management -->  Kibana -->  Saved Objects --> 点击右上角的 “Import” 按钮，选中 “export-v1.0.ndjson” 文件，点击右下角的导入按钮。

![import kibana objects](/images/2022-03-18_09-42-15.png)

用文本编辑器打开 filebeat.yml 配置文件，确保在 ` filebeat.inputs:` 这个部分中只包含下面的采集项目概况的代码。

```yml
- type: httpjson
  interval: 6m
  config_version: 2
  request.url: https://api.github.com/repos/ansible/awx
  request.method: GET
  # 在请求中增加个人认证令牌，提高数据获取条数限制
  request.transforms:
    - set:
        target: header.Authorization
        value: 'token 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'  #此处的token示例需要被替换
  # 使用数据处理器，修整数据
  processors:
    # 优化分析：增加方便搜索分析的字段
    - add_fields:
        fields:
          project:  awx
          kpi: overall
    # 解码裸json结果：json 内容解码为多个字段
    - decode_json_fields:
        fields: ["message"]
        target: "json"
```

以上代码中 token 后面的字符串需要替换为你自己的，获取方式在前往说过。

重点解释几个参数：

* type: httpjson ： 这是使用　Filebeat　的　httpjson　采集模块，下面整段都是它的配置，每一段采集不同的 GitHub API。
* intervel：6m ： 配置这个采集条目的采集频率，如果返回数据变化频率不高，每天采集两三次就行了。60m 是 60 分钟采集一次 的意思，可以配置为 60 分钟的 n 倍，GitHub API 限流的周期也是按 60 分钟恢复一次的。数据量大的时候需要考虑。
* request.url： 这里配置的是不同 GitHub API 的路径，其中包括了目标分析代码库的全路径，需要在采集前用浏览器测试其正确性。如果考虑到采集可能受限制，可以查阅 API 文档，适当提高 page size 的大小，从而节省采集请求次数。
* request.transforms: 配置了使用 token 认证方式，取保 filebeat 使用你的个人访问令牌，以认证用户的身份采集数据，否则非认证用户 60 次的请求限制，分分钟就会被用完。
* add_fields: 这一段配置了开源项目的名称，和本段采集器采集的数据分类；这里使用手工配置的方式，在后续增加其它类型数据采集的时候，还需要修改 project： 字段的项目名称。

打开命令行工具，进入 Filebeat 解压缩后的目录中。运行下面的命令 `./filebeat -e`。

```sh
➜  filebeat-7.17.0-darwin-x86_64 ./filebeat -e
```

这条命令的输出日志应该很快就会停止。 按 `ctrl + c` 结束首次数据采集测试。在没有报错的情况下，初始化信息采集就成功了。下面开始在 Kibana 里确认数据采集结果。

![](/images/2022-03-18_10-13-40.png)

点击 Kibana 左侧菜单：选择： Analytics  -->  Discovery -->  右上角的 Open 菜单 --> 选择 overall 预制查询视图。在数据显示视图中，至少应该有一条数据，打开这条数据观察所有采集到的字段。项目 Overall 的数据信息丰富，变化率不大，可以每天两三次，就能够绘制出相关的趋势分析图表。

虽然只采集到有限的项目概况数据，现在已经可以查看 GitHub 开源项目分析看板了。

点击 Kibana 左侧菜单：选择： Analytics  -->  Dashboard --> 点击 GitHub Project Analysis 。

![](/images/2022-03-18_10-25-43.png)

查看相关信息点：

1. 在项目选择菜单中，选中 filebeat.yml 文件中所配置的项目名称。界面中的数据会自动刷新。
2. 在 overall mertrics 这个区域里显示了基础的项目数据。
3. 这里显示了项目的年龄。这是个扩展字段，如果没有显示正确数值的话，应该排查解决后，在进行后续的采集和分析。

在我们分析一个开源项目的时候，完全不能只看 overall metrics 哪里的概况基础数据。可以参考 **DORA 2019 Report** 的分析报告，点击那个链接打开相关的说明文档。这是一个长期的跟踪调查，从四个维度分析软件交付效能。我将每个分析维度和 GitHub 中的相关开发活动数据做了对应。

| No. | 类型 | 名称 | GitHub 对应的开发活动数据 |
| ------------- | ------------- | ------------- | ------------- |
|   T1  |  吞吐量 |  部署（发布）频率  | 从主干交付一个正式发布版本，观察 GitHub 上项目 release 的频次，反应了产品的整体推进速率 |
|   T2  |  吞吐量 |  变更前置时间  | 任一 Pull Request 都包含修改、评审和合并的过程，度量 pr 从创建到关闭的周期，就可以看出代码参与各方的协作效率 |
|   S1  |  稳定性 |  服务恢复时间|  issues 很可能对应这用户环境（线上服务）的问题。任一 issues 的解决都可能关联 0 ~ n 个 pr，分析 issues 从创建到关闭的时间周期，能分析出该项目的问题处理能力 |
|   S2  |  稳定性 |  Change failure rate  | 失败的版本发布或者代码修复（pr）都会导致 issues 的扎堆创建，可以观察在新的 release 之后，或者一波 PR 关闭之后，有没有发生 issues 数量剧增的情况 |

值得说明的是，这四个维度的正向和反向分析都是悖论，可以导向完全不同的结果，需要理性判断。例如：某个代码库长期以来，保持着很高的 open issues 的状态，open issue 的数量高达 1500 以上。这个数据可能的分析结果如下：

* 这是一个非常火爆的项目：遭到用户热捧，参与者积极踊跃的参与项目反馈。
* 这个项目的技术债慎重：用户对这个坑很多，很深的项目，保持无法舍弃，长期吐槽的状态。
* 项目核心开发团队（含外部贡献者）专注创新：无法平衡新功能发布和技术债清理的关系，无奈选择了前者。
* 原作者公司已经转移阵地：对于一个长期且负责的项目而言，解铃还须系铃人，外部贡献者和其它开发者很难接盘或者挽救一个这样的项目。

可以在 Kibana 的，时间选择控件中，选择最近三个月、半年、一年等时段定向分析，也可以选择过去的某一年或者半年做横向 YOY 的对比分析。特别是对 release、pr 和 issues 三类数据的综合分析，我们不难从开发效能的角度度量开源项目的整体实力。本教程为大家抓取和分析数据提供了支撑。对项目的深度分析提供了角度。根据不同的项目分析需求，我们从这些数据中还可以开发出分析角度和图表。我为此在 GitHub 上也创建了一个项目，欢迎大家一起交流起来。

项目地址： <https://github.com/martinliu/sdp-dashboard>

在进入大批量数据采集前，还需要微调 Elasticsearch 后台的索引字段数量上线。点击展开左侧菜单，选择： Management -->  Dev Tools --> Console。 在这个开发者工具的左侧输入并执行下面的代码。

```json
PUT filebeat-7/_settings
{
  "index.mapping.total_fields.limit": 5000
}
```

执行的效果如下图所示。

![](/images/2022-03-18_12-17-52.png)

操作步骤：

1. 粘贴入以上代码后，点击右三角的执行按钮。
2. 右侧结果返回为： true ，表明成功更新了索引配置。

{{< panel status="success" title="自查点" >}}
需要检查和确认：1）Kibana 里正常导入了所有示例可视化控件和基础配置；2）filebeat 可执行文件可以正常的执行；3）完成了Elasticsearch 的索引初始化和配置微调。
{{< /panel >}}

在以上自测点都成功以后，在进入下面的步骤。

### 分类采集项目其它数据

在成功采集了项目概况数据以后，我们接下逐步采集和分析其它类型的数据包括，但不限于下列数据：

* contributors: 此 API 会贡献值倒序的方式返回贡献者清单，数据条数不一定完整，但是我们最多能关注前 30 的贡献者，排名靠后的大量贡献者，都是非活动的，贡献次数为10 以内的非关键人员。很可能这个清单是无法访问的，项目所属公司对其限制了访问权限。其他 API 也可能有这个问题。
* releases : 关键重要指标数据，但是不是所有项目都很专业的从一开始就维护 reelase 清单，有些项目会记录每个 release 的下载次数和点赞互动次数，有些项目不跟踪下载次数。
* languages : 非关键重要数据。
* tags : 分关键重要数据
* issues ： 极其关键且重要数据，对于比较火热的项目数量比较大，多则几万条。
* pulls： pull rquest 也是极其关键且重要数据，值得深度分析。火热项目数据量较大。

顶级开源项目的全量数据文档数会比较轻易的超过 5 万个。

参考下面的步骤采集以上每一类数据：

1. 从示例 filebeat.yml 配置文件中，复制一段目标的配置参数段落，粘贴到测试机的 Filebeat 配置文件中。
2. 修改 request.url ， token 和 fields.project 等关键字段。
3. 在浏览器中再次确认修改后的 request.url 是否能正常访问，排除 GitHub 网站服务不好的情况。或者在命令行里使用 curl 命令测试。
4. 运行 `./filebeat test output` 测试后端 Elasticsearch 服务器的服务是否正常。
5. 运行 `./filebeat -e` 开始当前这个新分类的数据采集
6. 等待命令行日志停止滚动，除了 issues 和 pulls 之外，其它分类数据，应该在分钟级别的就可以完成首次采集。对于数据量大的项目，issues 和 pulls 都可能会耗时一小时，或者更长时间。
7. 采集静止的时候， 按 `ctrl + c` 结束数据采集。
8. 在 Kibana 中，进入 Discovery ，点击 open 菜单，选中对应的查询视图；可能需要先调整一下时间控件的时间设置，例如选择最近一年，观察数据在时间上的分布，观察采集到了多少条该类数据，将采集到的文档数与在 GitHub 上该项目页面上的数据做比对确认。也很可能需要多个采集周期才能追平这类数据。
9. 打开单个文档，观察和熟悉，这类数据都有那些字段，思考那些是值得分析的字段，可以将其添加到当前的数据表格中；对于 issues 和 pull 数据文档，还需要观察 `project-age` 这个实时计算字段的数值，它是一个分钟计数器，状态为 closed 后就不会变化。
10. 尝试用右上角的时间选择控件改变时间分析区间，观察其他时间分析区间的数据分布特性和数值。
11. 返回 Dashboard --> GitHub Project Analysis 开源项目分析看板，查看相关的数据分析图表，也可能需要微调某数据显示控件。选择不同的时间跨度，浏览项目数据的完整性。有些项目概况数据是保持不变的。

## 定制项目数据分析看板

### 项目数据重置

### 修订示例数据显示控件

### 用 Lens 创建新数据分析


## 总结